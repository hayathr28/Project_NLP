{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfc6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hussain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hussain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hussain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Hussain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk, re, string\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, recall_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6246c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18652\\189615863.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c661a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8410306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf434a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark')\n",
    "sns.countplot(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f5282",
   "metadata": {},
   "outputs": [],
   "source": [
    " # creating new column for storing the length of reviews\n",
    "df['length'] = df['text'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].plot(bins=50, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca440dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['length'] == 157]['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ceb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='length', by='target', bins=50, figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2061481",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[df['target']==1]\n",
    "df_0 = df[df['target']==0]\n",
    "df_1['text'] = df_1['text'].apply(remove_stopwords)\n",
    "df_0['text'] = df_0['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d749a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "plt.figure(figsize=(20,20)) \n",
    "wc = WordCloud(max_words=1000,width=1600,height=800).generate(\" \".join(df_1.text))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words=1000, width=1600, height=800).generate(\" \".join(df_0.text))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "#creating a list of possible stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def cleanTweet(text):\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    #removing stop words & lemmatizing the words\n",
    "    words = \" \".join([lemma.lemmatize(word) for word in words if word not in (stop)])\n",
    "    text = \"\".join(words)\n",
    "    #removing non-alphabetic characters\n",
    "    text = re.sub('[^a-z]', ' ', text)\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99301a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweets'] = df['text'].apply(cleanTweet)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e52936",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target\n",
    "X = df.cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfdif vectorizer-> bi-gram-> pair of words\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, ngram_range=(1,2))\n",
    "tfidf_train_2 = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test_2 = tfidf_vectorizer.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tf = MultinomialNB()\n",
    "mnb_tf.fit(tfidf_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "scoring  = 'accuracy'\n",
    "\n",
    "acc_mnb2 = cross_val_score(estimator = mnb_tf, X = tfidf_train_2, y = y_train, cv = kfold, scoring=scoring)\n",
    "acc_mnb2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01888379",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mnb2 = mnb_tf.predict(tfidf_test_2)\n",
    "CM = confusion_matrix(y_test, pred_mnb2)\n",
    "sns.heatmap(CM, cmap='Blues', linecolor='black', linewidth=1, annot=True, fmt='', xticklabels = ['Normal', 'Disaster'], yticklabels = ['Normal', 'Disaster'])\n",
    "\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "\n",
    "specificity = TN/(TN+FP)\n",
    "acc = accuracy_score(y_test, pred_mnb2)\n",
    "prec = precision_score(y_test, pred_mnb2)\n",
    "rec = recall_score(y_test, pred_mnb2)\n",
    "f1 = f1_score(y_test, pred_mnb2)\n",
    "\n",
    "model_results = pd.DataFrame([['Multinommial Naive Bayes - TFIDF-Bigram', acc, prec, rec, specificity, f1]],\n",
    "                            columns = ['Model', 'Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score'])\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abaa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_tf = PassiveAggressiveClassifier()\n",
    "pass_tf.fit(tfidf_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10)\n",
    "scoring = 'accuracy'\n",
    "acc_pass2 = cross_val_score(estimator = pass_tf, X = tfidf_train_2, y = y_train, cv = kfold, scoring=scoring)\n",
    "acc_pass2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f46780",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pass2 = pass_tf.predict(tfidf_test_2)\n",
    "CM = confusion_matrix(y_test, pred_pass2)\n",
    "sns.heatmap(CM, cmap='Blues', linecolor='black', linewidth=1, annot=True, fmt='', xticklabels = ['Normal', 'Disaster'], yticklabels = ['Normal', 'Disaster'])\n",
    "\n",
    "acc = accuracy_score(y_test, pred_pass2)\n",
    "prec = precision_score(y_test, pred_pass2)\n",
    "rec = recall_score(y_test, pred_pass2)\n",
    "f1 = f1_score(y_test, pred_pass2)\n",
    "\n",
    "results = pd.DataFrame([['Passive Agressive - TFIDF-Bigram', acc, prec, rec, specificity, f1]],\n",
    "                            columns = ['Model', 'Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score'])\n",
    "results = model_results.append(results, ignore_index = True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baab2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_3 = TfidfVectorizer(stop_words = 'english', max_df=0.8, ngram_range=(1,3))\n",
    "tfidf_train_3 = tfidf_vectorizer_3.fit_transform(X_train)\n",
    "tfidf_test_3 = tfidf_vectorizer_3.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tf3 = MultinomialNB()\n",
    "mnb_tf3.fit(tfidf_train_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acabaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10)\n",
    "scoring = 'accuracy'\n",
    "acc_mnb3 = cross_val_score(estimator=mnb_tf, X = tfidf_train_3, y=y_train, cv=kfold, scoring=scoring)\n",
    "acc_mnb3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mnb3 = mnb_tf3.predict(tfidf_test_3)\n",
    "CM = confusion_matrix(y_test, pred_mnb3)\n",
    "sns.heatmap(CM, cmap='Blues', linecolor='black', linewidth=1, annot=True, fmt='', xticklabels = ['Normal', 'Disaster'], yticklabels = ['Normal', 'Disaster'])\n",
    "\n",
    "acc = accuracy_score(y_test, pred_mnb3)\n",
    "prec = precision_score(y_test, pred_mnb3)\n",
    "rec = recall_score(y_test, pred_mnb3)\n",
    "f1 = f1_score(y_test, pred_mnb3)\n",
    "\n",
    "mod_results = pd.DataFrame([['Multinomial Naive Bayes - TFIDF-Trigram', acc, prec, rec, specificity, f1]],\n",
    "                            columns = ['Model', 'Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score'])\n",
    "results = results.append(mod_results, ignore_index = True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_tf3 = PassiveAggressiveClassifier()\n",
    "pass_tf3.fit(tfidf_train_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c25187",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10)\n",
    "scoring = 'accuracy'\n",
    "\n",
    "acc_pass3 = cross_val_score(estimator=pass_tf3, X = tfidf_train_3, y=y_train, cv=kfold, scoring=scoring)\n",
    "acc_pass3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pass3 = pass_tf3.predict(tfidf_test_3)\n",
    "CM = confusion_matrix(y_test, pred_pass3)\n",
    "sns.heatmap(CM, cmap='Blues', linecolor='black', linewidth=1, annot=True, fmt='', xticklabels = ['Normal', 'Disaster'], yticklabels = ['Normal', 'Disaster'])\n",
    "\n",
    "acc = accuracy_score(y_test, pred_pass3)\n",
    "prec = precision_score(y_test, pred_pass3)\n",
    "rec = recall_score(y_test, pred_pass3)\n",
    "f1 = f1_score(y_test, pred_pass3)\n",
    "\n",
    "mod1_results = pd.DataFrame([['Passive Agressive - TFIDF-Trigram', acc, prec, rec, specificity, f1]],\n",
    "                            columns = ['Model', 'Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score'])\n",
    "results = results.append(mod1_results, ignore_index = True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):\n",
    "   \n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "    \n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels, coef, feat)\n",
    "    print()\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_informative_feature_for_binary_classification(tfidf_vectorizer_3, pass_tf3, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Just happened a terrible car crash\",\n",
    "    \"Heard about #earthquake is different cities, stay safe everyone.\",\n",
    "    \"No I don't like cold\",\n",
    "    \"@RosieGray Now in all sincerety do you think the UN would to Israel if there was a fraction of chance of being annihilated\"\n",
    "    ]\n",
    "\n",
    "tfidf_trigram = tfidf_vectorizer_3.transform(sentences)\n",
    "\n",
    "predictions = pass_tf3.predict(tfidf_trigram)\n",
    "\n",
    "for text, label in zip(sentences, predictions):\n",
    "    if label==1:\n",
    "        target = \"Disaster Tweet\"\n",
    "        print(\"text: \", text, \"\\nClass\", target)\n",
    "        print()\n",
    "    else:\n",
    "        target = \"Normal Tweet\"\n",
    "        print(\"text: \", text,\"\\nClass\", target)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7950b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
